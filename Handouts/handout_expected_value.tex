\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{hyperref}
\let\oldemptyset\emptyset
\let\emptyset\varnothing


\title{\textbf{Esssentials of Applied Data Analysis\\
				IPSA-USP Summer School 2017}\newline\\
				Handout - Expected Value}

\author{Leonardo Sangali Barone\\ \href{leonardo.barone@usp.br}{leonardo.barone@usp.br}}
\date{jan/17}

\begin{document}

\maketitle

	\subsection*{Expectation and mean value of a discrete random variable}

	\subsection*{Expectation and mean value of a discrete random variable}
	Imagine a game where you toss a coin and get \$1 if the result is head and 0 if the result is tail. How much should you expect to earn?
\newline\\
	Imagine a game where you roll a dice and you can get \$1 times the number you get on the dice. How much should you expect to earn?

	\subsection*{Expectation and mean value of a discrete random variable}
	The expected value of a discrete random variable can be easily obtained by summing each result multiplied by probability of that result ocurring.  
	\subsection*{Expectation and mean value of a discrete random variable}
	Imagine a game where you toss a coin and get \$1 if the result is head and 0 if the result is tail. How much should you expect to earn?
	\[E[X] = 0.5 * 0 + 0.5 * 1 = 0.5\]
	Imagine a game where you roll a dice and you can get \$1 times the number you get on the dice. How much should you expect to earn?	
\small{\[E[X] = \frac{1}{6} * 1 + \frac{1}{6} * 2 + \frac{1}{6} * 3 + \frac{1}{6} * 4 + \frac{1}{6} * 5 + \frac{1}{6} * 6 + \frac{1}{6} * 1 = 3.666\]}


	\subsection*{Expectation and mean value of a discrete random variable}
	In more general term, the expectation or mean of a discrete random variable is:
	\[E[X] = \sum\limits_{i=1}^n x_i * P(X = x_i) = \sum\limits_{i=1}^n x_i * f(x_i)\]
where $x_i$ is an occurence of the variable X and $f(x_i)$ is the probability mass function (the probability that $x_i$ will occur).

	\subsection*{Expectation and mean value of a discrete random variable}
	Note that, since the set of all $x_i$ is the set of all possible values for $X$, then
	\[E[X] = \sum\limits_{i=1}^n P(X = x_i) = \sum\limits_{i=1}^n f(x_i) = 1\]


	\subsection*{Expectation and mean value of a discrete random variable}
	Example: (Made-up) survey with 2000 respondents in 2014 Brazilian presidential elections.
	\newline\\
		\begin{tabular}{|c|c|c|}
\hline
	Candidate $(x)$ & \# of respondents & $P(X=x)$\\
	\cline{1-3}
	Dilma & 800  & 0.40\\
	AÃ©cio &  500 & 0.25\\
	Marina & 400 & 0.20\\
	Other/Null/DK & 500 & 0.25 \\
	\cline{1-3}
	Total & 2000 & 1.00\\
\hline
\end{tabular}
	\newline\\
	Can we calculate $E[X]$?


	\subsection*{Expectation and mean value of a discrete random variable}
	When all $P(X=x_i)$ is the same for every $x_i$, we can simplify the expression of $E[X]$ to:
	\[E[X] = \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{x_1 + x_2 + ... + x_n}{n}\]
	which is what we normally do to calculate avareges in daily life.

	\subsection*{Expectation and mean value of a discrete random variable}
	Some properties of the mean:
	\[E[a*X] = a * E[X]\]
	\[E[X+ b] = E[X] + b\]
	So what? Well, if you multiply a variable by a number ($a$) to generate a new variable, the mean of the new variable is the mean of the old variable times $a$.
	\newline\\Also, if you sum a quantity $b$ to a random variable, the mean of the result variable will be the mean of the original variable plus $b$.
	\newline\\Let's try it later on an statistical software!

	\subsection*{Expectation and variance of a discrete random variable}
	Another important quantity of a random variable is the variance. The name is self-explanatory: the variance measures how spread-out a variable is.

	\subsection*{Expectation and variance of a discrete random variable}
	The variance of a random variable is also an expectation:
		\[Var[X] = \sum\limits_{i=1}^n[x_i - E[X]]^2 * P(X=x_i) =\sum\limits_{i=1}^n[x_i - E[X]]^2 * f(x_i)\]
where $x_i$ is an occurence of the variable X, E[X] is the expected value of X and $f(x_i)$ is the probability mass function (the probability that $x_i$ will occur).

	\subsection*{Expectation and variance of a discrete random variable}
	Coin game (where heads pays off \$1 and tails pays off \$0):
	\[Var[X] = \sum\limits_{i=1}^n[x_i - E[X]]^2 * f(x_i) = [0.5 - 0.5]^2 * 0 + 0.5 * 1 = 0.5\]


	\subsection*{Expectation and variance of a discrete random variable}
	Some properties of the variance:
	\[Var[a*X] = a^2 * E[X]\]
	\[Var[X+ b] = Var[X]\]
	So what? Well, if you multiply a variable by a number ($a$) to generate a new variable, the variance of the new variable is the variance of the old variable times $a^2$.
	\newline\\Also, if you sum a quantity $b$ to a random variable, the variance of the result variable will equal to the variance of the original variable.
	\newline\\Let's try it later on an statistical software!

	\subsection*{Expectation and variance of a discrete random variable}
	Some properties of the variance:
	\[Var[a*X] = a^2 * E[X]\]
	\[Var[X+ b] = Var[X]\]
	So what? Well, if you multiply a variable by a number ($a$) to generate a new variable, the variance of the new variable is the variance of the old variable times $a^2$.
	\newline\\Also, if you sum a quantity $b$ to a random variable, the variance of the result variable will equal to the variance of the original variable.
	\newline\\Let's try it later on an statistical software!

	\subsection*{Expectation, mean, variance and standard deviation}
	Notation:
	\[E[X] = \mu[X] = \mu\]
	\[Var[X] = \sigma^2[X] = \sigma^2\]
	The standard deviation ($\sigma$) of a variable is
	\[\sigma = \sqrt{Var[X]} = \sqrt{\sigma^2}\]
	Another way to calculate the variance is simply doing: 
	\[Var[X] = E[X^2] - (E[X])^2\]	

	\subsection*{Continuous random variables}
	The rules that apply to discrete random variables also apply to continous random variables.
	\newline\\ The main problem when we deal with a continuous random variable is that we cannot count every possible outcome and multiply it by the probability of that outcome occurin (remember: continous variables are and infinite set and uncontable!)

	\subsection*{Expectation and variance of a continuous random variable}
	In other words, we cannot do this:
	\[\sum\limits_{i=1}^n P(X = x_i) = \sum\limits_{i=1}^n f(x_i) = 1\]
or this
		\[E[X] = \sum\limits_{i=1}^n x_i* P(X=x_i) =\sum\limits_{i=1}^n x_i * f(x_i)\]
or this
		\[Var[X] = \sum\limits_{i=1}^n[x_i - E[X]]^2 * P(X=x_i) =\sum\limits_{i=1}^n[x_i - E[X]]^2 * f(x_i)\]
		because can't count every $x_i$. What do we do instead?

	\subsection*{Expectation and variance of a continuous random variable}
	But we can do this (see Moore and Siegel, chap 7 for integrals)
	\[\int_{-\infty}^{\infty} \text{f(x) dx} = 1\]
and this
		\[E[X] = \int_{-\infty}^{\infty}  \text{x f(x) dx}\]
and this
		\[Var[X] = \int_{-\infty}^{\infty}  [x - E[X]]^2 \text{f(x) dx}\]


	\subsection*{Functions of random variables}
	We can sum random variables. For example: $Z=X+Y$
	\newline\\
	What is the expected value of $Z$?	
	\[E[Z] = E[X+Y] = E[X] + E[Y]\]


	\subsection*{Functions of random variables}
	We can multiply random variables. For example: $Z=X*Y$
	\newline\\
	What is the expected value of $Z$? It dependes. If $X$ and $Y$ are independent of each other, then	
	\[E[Z] = E[X*Y] =E[XY] = E[X] * E[Y]\]
	What if they are not independent?

	\subsection*{Functions of random variables - covariance}
	The covariance of a variable is defined as
	\[Cov(X,Y) = E[(X-E[X]) * (Y-E[Y])]\]
	But wait, this looks familiar!
	\[Cov(X,X) = E[(X-E[X]) * (X-E[X])] = \]
	\[= E[(X-E[X])^2 = E[X^2] - (E[X])^2 = Var[X]\]


	\subsection*{Functions of random variables - covariance}
	Another way to look at the covariance 
	\[Cov(X,Y) = E[XY] - E[X]*E[Y]\]
	Wait again! Isn't $E[XY]$ the expected value of the multiplication of X$X$ and $Y$? Yes it is, so we can rearrange the formula to obtain:
	\[E[XY] = E[X]*E[Y] + Cov(X,Y)\]
	If $X$ and $Y$ are independent of each other, then 
	\[E[XY] = E[X]*E[Y]\]
	Or we can simply say that
	\[Cov(X,Y) = 0\]

	\subsection*{Functions of random variables - covariance}
	So the covariance is a measure of how two variables are related to each other. One problem with the covariance is that is can be any number, hence, we can't compare two covariances. But there is a way to "standardize" covariances..

	\subsection*{Functions of random variables - correlation}
	The correlation of two variables is define as:
	\[\rho_xy = Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_x*\sigma_y} \]
	Where $\sigma_x$ and $\sigma_y$ are the standard deviations of $X$ and $Y$.

	\subsection*{Functions of random variables - correlation}
	One important property of the correlation is that, differently from the covariance, it is bounded
	\[ -1 \leq Corr(X,Y) \leq 1\]
	As we will see in the future, correlation is a measure of linear relation among two random variables.

	\subsection*{Functions of random variables - variance}
	Finally, when we sum two variables, for example: $Z=X+Y$, the variance of the sum is:
	\[Var[Z] = Var[X+Y] = Var[X] + Var[Y] + 2*Cov(X,Y)\]
	Since $Cov(X,Y) = 0$ under indepence, if $X$ and $Y$ are indepent we can simplify the variance to:
	\[Var[Z] = Var[X+Y] = Var[X] + Var[Y]\]




\end{document}
